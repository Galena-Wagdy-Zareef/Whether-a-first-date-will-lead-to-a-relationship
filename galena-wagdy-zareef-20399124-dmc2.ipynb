{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"**Whether a first date will lead to a relationship**\npredicting the outcome of a specific speed dating session based on the profile of two people, by implementing a recommendation system to better match people in speed dating events.This is a binary classification task. Given a data sample (information about the dating session), by predicting the probability (0-1, float) that the dating session will lead to a successful match.\nThe data is 191 columns as an input.\nthe output column (label) is match column, which is binary classification.\npredict the probability that a dating session will lead to a successful match, you can use binary classification algorithms such as logistic regression, decision trees, random forests, support vector machines (SVMs), and neural networks\nthe challenges are Missing data: The dataset contains missing data that you will need to handle appropriately.\nImbalanced classes: The dataset might have imbalanced classes, which can make it difficult to train a model that performs well on both classes.\nFeature selection: You might need to perform feature selection to identify the most important features for predicting the probability of success.\nOverfitting: You might need to use techniques such as cross-validation and regularization to prevent overfitting of your model.\nThe impact of using this dataset to predict the probability of success for dating sessions could be significant. For example, it could help dating apps and websites to better match people based on their interests and preferences, leading to more successful matches and happier users.\nHowever, itâ€™s important to note that there are limitations to this dataset and the models that can be built from it. For example, the dataset only includes information about speed dating events, which might not be representative of all dating scenarios.\nAn ideal solution would be a model that can accurately predict the probability of success for dating sessions based on the available data in the dataset.\nTo achieve this, you might need to use a combination of binary classification algorithms and techniques such as feature selection, cross-validation, and regularization to build a model that performs well on both classes.","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-03-30T08:48:16.512317Z","iopub.execute_input":"2023-03-30T08:48:16.513114Z","iopub.status.idle":"2023-03-30T08:48:16.522864Z","shell.execute_reply.started":"2023-03-30T08:48:16.513071Z","shell.execute_reply":"2023-03-30T08:48:16.521625Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.datasets import fetch_openml\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV, PredefinedSplit\nfrom xgboost.sklearn import XGBClassifier\nfrom sklearn.datasets import make_classification\nfrom sklearn.feature_selection import VarianceThreshold\nfrom skopt import BayesSearchCV\nfrom skopt.space import Real, Categorical, Integer\nfrom sklearn.svm import SVC\nimport xgboost as xgb\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn import metrics\nfrom sklearn.metrics import roc_curve\nimport joblib\nfrom sklearn_pandas import DataFrameMapper\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.feature_extraction import DictVectorizer\nfrom sklearn.pipeline import FeatureUnion\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.compose import make_column_transformer\nfrom xgboost import XGBRegressor\nfrom sklearn.preprocessing import OneHotEncoder","metadata":{"execution":{"iopub.status.busy":"2023-03-30T10:41:35.541174Z","iopub.execute_input":"2023-03-30T10:41:35.541879Z","iopub.status.idle":"2023-03-30T10:41:35.551682Z","shell.execute_reply.started":"2023-03-30T10:41:35.541843Z","shell.execute_reply":"2023-03-30T10:41:35.550449Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/cisc-873-dm-w23-a2/train.csv',index_col=0)\nprint(type(df))\nprint(df.shape)\ndf.head(5)","metadata":{"execution":{"iopub.status.busy":"2023-03-30T08:48:18.472419Z","iopub.execute_input":"2023-03-30T08:48:18.474850Z","iopub.status.idle":"2023-03-30T08:48:18.738994Z","shell.execute_reply.started":"2023-03-30T08:48:18.474798Z","shell.execute_reply":"2023-03-30T08:48:18.737996Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.columns","metadata":{"execution":{"iopub.status.busy":"2023-03-30T08:48:18.740607Z","iopub.execute_input":"2023-03-30T08:48:18.741254Z","iopub.status.idle":"2023-03-30T08:48:18.750164Z","shell.execute_reply.started":"2023-03-30T08:48:18.741216Z","shell.execute_reply":"2023-03-30T08:48:18.749009Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.describe()","metadata":{"execution":{"iopub.status.busy":"2023-03-30T08:48:18.756836Z","iopub.execute_input":"2023-03-30T08:48:18.757767Z","iopub.status.idle":"2023-03-30T08:48:18.764661Z","shell.execute_reply.started":"2023-03-30T08:48:18.757727Z","shell.execute_reply":"2023-03-30T08:48:18.763160Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.info()","metadata":{"execution":{"iopub.status.busy":"2023-03-30T08:48:18.766438Z","iopub.execute_input":"2023-03-30T08:48:18.766832Z","iopub.status.idle":"2023-03-30T08:48:18.777282Z","shell.execute_reply.started":"2023-03-30T08:48:18.766792Z","shell.execute_reply":"2023-03-30T08:48:18.776022Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.dtypes","metadata":{"execution":{"iopub.status.busy":"2023-03-30T08:48:18.779014Z","iopub.execute_input":"2023-03-30T08:48:18.779725Z","iopub.status.idle":"2023-03-30T08:48:18.787363Z","shell.execute_reply.started":"2023-03-30T08:48:18.779684Z","shell.execute_reply":"2023-03-30T08:48:18.785888Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = df.drop_duplicates()","metadata":{"execution":{"iopub.status.busy":"2023-03-30T08:48:18.788936Z","iopub.execute_input":"2023-03-30T08:48:18.790176Z","iopub.status.idle":"2023-03-30T08:48:18.892566Z","shell.execute_reply.started":"2023-03-30T08:48:18.790134Z","shell.execute_reply":"2023-03-30T08:48:18.891239Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.columns[df.isnull().any()]","metadata":{"execution":{"iopub.status.busy":"2023-03-30T08:48:18.898126Z","iopub.execute_input":"2023-03-30T08:48:18.900816Z","iopub.status.idle":"2023-03-30T08:48:18.907153Z","shell.execute_reply.started":"2023-03-30T08:48:18.900751Z","shell.execute_reply":"2023-03-30T08:48:18.906004Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['match'].isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2023-03-30T08:48:18.911522Z","iopub.execute_input":"2023-03-30T08:48:18.912369Z","iopub.status.idle":"2023-03-30T08:48:18.921720Z","shell.execute_reply.started":"2023-03-30T08:48:18.912331Z","shell.execute_reply":"2023-03-30T08:48:18.920258Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df= df.fillna(0)","metadata":{"execution":{"iopub.status.busy":"2023-03-30T08:48:18.931226Z","iopub.execute_input":"2023-03-30T08:48:18.931615Z","iopub.status.idle":"2023-03-30T08:48:18.941080Z","shell.execute_reply.started":"2023-03-30T08:48:18.931573Z","shell.execute_reply":"2023-03-30T08:48:18.939006Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.eq(0).sum(axis=0)","metadata":{"execution":{"iopub.status.busy":"2023-03-30T08:48:18.943943Z","iopub.execute_input":"2023-03-30T08:48:18.946811Z","iopub.status.idle":"2023-03-30T08:48:18.953515Z","shell.execute_reply.started":"2023-03-30T08:48:18.946773Z","shell.execute_reply":"2023-03-30T08:48:18.952415Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.loc[:,df.dtypes == np.object].columns","metadata":{"execution":{"iopub.status.busy":"2023-03-30T08:48:18.955624Z","iopub.execute_input":"2023-03-30T08:48:18.956578Z","iopub.status.idle":"2023-03-30T08:48:18.964319Z","shell.execute_reply.started":"2023-03-30T08:48:18.956523Z","shell.execute_reply":"2023-03-30T08:48:18.963140Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.loc[:,df.dtypes == np.object]","metadata":{"execution":{"iopub.status.busy":"2023-03-30T08:48:18.966516Z","iopub.execute_input":"2023-03-30T08:48:18.968126Z","iopub.status.idle":"2023-03-30T08:48:18.975338Z","shell.execute_reply.started":"2023-03-30T08:48:18.968089Z","shell.execute_reply":"2023-03-30T08:48:18.974202Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['mn_sat'] = pd.to_numeric(df['mn_sat'], errors='coerce')\ndf['tuition'] = pd.to_numeric(df['tuition'], errors='coerce')\ndf['zipcode'] = pd.to_numeric(df['zipcode'], errors='coerce')\ndf['income'] = pd.to_numeric(df['income'], errors='coerce')","metadata":{"execution":{"iopub.status.busy":"2023-03-30T08:48:18.978321Z","iopub.execute_input":"2023-03-30T08:48:18.980272Z","iopub.status.idle":"2023-03-30T08:48:19.032627Z","shell.execute_reply.started":"2023-03-30T08:48:18.980219Z","shell.execute_reply":"2023-03-30T08:48:19.031280Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['field'] = df['field'].astype(\"category\")\ndf['undergra'] = df['undergra'].astype(\"category\")\ndf['from'] = df['from'].astype(\"category\")\ndf['career'] = df['career'].astype(\"category\")","metadata":{"execution":{"iopub.status.busy":"2023-03-30T08:48:19.033855Z","iopub.execute_input":"2023-03-30T08:48:19.034283Z","iopub.status.idle":"2023-03-30T08:48:19.073391Z","shell.execute_reply.started":"2023-03-30T08:48:19.034240Z","shell.execute_reply":"2023-03-30T08:48:19.071991Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y = df['match']\nX = df.drop('match', axis=1)\n# print('original shape', X.shape, y.shape)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n# print('y_test', y_test)","metadata":{"execution":{"iopub.status.busy":"2023-03-30T08:48:19.078485Z","iopub.execute_input":"2023-03-30T08:48:19.081543Z","iopub.status.idle":"2023-03-30T08:48:19.116522Z","shell.execute_reply.started":"2023-03-30T08:48:19.081499Z","shell.execute_reply":"2023-03-30T08:48:19.115197Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"features_numeric = list(X_train.select_dtypes(include=['float64', 'int64']))\n\nfeatures_categorical = list(X_train.select_dtypes(include=['category']))\n\n# print('numeric features:', features_numeric)\n# print('categorical features:', features_categorical)","metadata":{"execution":{"iopub.status.busy":"2023-03-30T08:48:19.122575Z","iopub.execute_input":"2023-03-30T08:48:19.126183Z","iopub.status.idle":"2023-03-30T08:48:19.146532Z","shell.execute_reply.started":"2023-03-30T08:48:19.126130Z","shell.execute_reply":"2023-03-30T08:48:19.145319Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.random.seed(0)\ntransformer_numeric = Pipeline(\n    steps=[\n        ('imputer', SimpleImputer()),\n        ('scaler', StandardScaler())]\n)\n\ntransformer_categorical = Pipeline(\n    steps=[\n        ('imputer', SimpleImputer(strategy='constant')),\n        ('onehot', OneHotEncoder(handle_unknown='ignore'))\n    ]\n)\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', transformer_numeric, features_numeric),\n        ('cat', transformer_categorical, features_categorical)\n    ]\n)\n\nfull_pipline = Pipeline(\n    steps=[\n        ('preprocessor', preprocessor),\n        ('my_classifier', \n           XGBClassifier(),\n        )\n    ]\n)","metadata":{"execution":{"iopub.status.busy":"2023-03-30T10:16:56.264030Z","iopub.execute_input":"2023-03-30T10:16:56.264413Z","iopub.status.idle":"2023-03-30T10:16:56.272571Z","shell.execute_reply.started":"2023-03-30T10:16:56.264380Z","shell.execute_reply":"2023-03-30T10:16:56.271292Z"},"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"full_pipline = full_pipline.fit(X_train, y_train)\nfull_pipline.predict(X_test)","metadata":{"execution":{"iopub.status.busy":"2023-03-30T10:16:59.482653Z","iopub.execute_input":"2023-03-30T10:16:59.483040Z","iopub.status.idle":"2023-03-30T10:17:03.594228Z","shell.execute_reply.started":"2023-03-30T10:16:59.483006Z","shell.execute_reply":"2023-03-30T10:17:03.593356Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"param_grid = {\n    'preprocessor__num__imputer__strategy': ['mean'],\n    'my_classifier__n_estimators': [20, 30, 40],\n    'my_classifier__max_depth':[10, 20, 30],\n    \n}\n\ngrid_search = GridSearchCV(\n    full_pipline, param_grid, cv=2, verbose=1, n_jobs=2, \n    scoring='roc_auc')\n\ngrid_search.fit(X_train, y_train)\n\nprint('best score {}'.format(grid_search.best_score_))\nprint('best score {}'.format(grid_search.best_params_))","metadata":{"execution":{"iopub.status.busy":"2023-03-30T10:21:46.639824Z","iopub.execute_input":"2023-03-30T10:21:46.640750Z","iopub.status.idle":"2023-03-30T10:22:21.053911Z","shell.execute_reply.started":"2023-03-30T10:21:46.640700Z","shell.execute_reply":"2023-03-30T10:22:21.052826Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"grid_search.predict(X_test)","metadata":{"execution":{"iopub.status.busy":"2023-03-30T10:22:21.058360Z","iopub.execute_input":"2023-03-30T10:22:21.060485Z","iopub.status.idle":"2023-03-30T10:22:21.103566Z","shell.execute_reply.started":"2023-03-30T10:22:21.060443Z","shell.execute_reply":"2023-03-30T10:22:21.102816Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"grid_search = GridSearchCV(\n    full_pipline, param_grid, cv=5, verbose=1, n_jobs=2, \n    scoring='roc_auc')\n\ngrid_search.fit(X_train, y_train)\n\nprint('best score {}'.format(grid_search.best_score_))\nprint('best score {}'.format(grid_search.best_params_))","metadata":{"execution":{"iopub.status.busy":"2023-03-30T10:22:21.106994Z","iopub.execute_input":"2023-03-30T10:22:21.108812Z","iopub.status.idle":"2023-03-30T10:24:09.334017Z","shell.execute_reply.started":"2023-03-30T10:22:21.108780Z","shell.execute_reply":"2023-03-30T10:24:09.333073Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"random_search = RandomizedSearchCV(\n    full_pipline, param_grid, cv=5, verbose=1, n_jobs=2, \n    n_iter=10,\n    scoring='roc_auc')\n\nrandom_search.fit(X_train, y_train)\n\nprint('best score {}'.format(random_search.best_score_))\nprint('best score {}'.format(random_search.best_params_))","metadata":{"execution":{"iopub.status.busy":"2023-03-30T10:24:09.338359Z","iopub.execute_input":"2023-03-30T10:24:09.340491Z","iopub.status.idle":"2023-03-30T10:25:57.335774Z","shell.execute_reply.started":"2023-03-30T10:24:09.340453Z","shell.execute_reply":"2023-03-30T10:25:57.334911Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"SVC_pipline = Pipeline(\n    steps=[\n        ('preprocessor', preprocessor),\n        ('my_svc', SVC(class_weight='balanced',probability=True))\n    ]\n)\nbayes_search = BayesSearchCV(\n    SVC_pipline,\n    {\n        'my_svc__C': Real(1e-6, 1e+6, prior='log-uniform'),\n        'my_svc__gamma': Real(1e-6, 1e+1, prior='log-uniform'),\n        'my_svc__degree': Integer(1,8),\n        'my_svc__kernel': Categorical(['linear', 'poly', 'rbf']),\n    },\n\n    n_iter=3,\n    random_state=0,\n    verbose=1,\n    cv=5,\n)\n\nbayes_search.fit(X_train, y_train)\n\nprint('best score {}'.format(bayes_search.best_score_))\nprint('best score {}'.format(bayes_search.best_params_))\n","metadata":{"execution":{"iopub.status.busy":"2023-03-30T10:25:57.339244Z","iopub.execute_input":"2023-03-30T10:25:57.340017Z","iopub.status.idle":"2023-03-30T10:32:31.266248Z","shell.execute_reply.started":"2023-03-30T10:25:57.339984Z","shell.execute_reply":"2023-03-30T10:32:31.265013Z"},"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"transformer_numeric = Pipeline(\n    steps=[\n        ('imputer', SimpleImputer()),\n        ('scaler', StandardScaler())]\n)\n\ntransformer_categorical = Pipeline(\n    steps=[\n        ('imputer', SimpleImputer(strategy='constant')),\n        ('onehot', OneHotEncoder(handle_unknown='ignore'))\n    ]\n)\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', transformer_numeric, features_numeric),\n        ('cat', transformer_categorical, features_categorical)\n    ]\n)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xgb_cl = xgb.XGBClassifier(eval_metric='logloss', seed=7) \n# Create XGBoost pipeline:\nxgb_pipeline = Pipeline(steps=[\n    ('preprocess', preprocessor),\n    ('model', xgb_cl)\n])\n# Evaluate the model with the use of cv:\ncv = StratifiedKFold(n_splits=5, shuffle=True, random_state=7)  #, shuffle=True with or without shuffle??\nscores = cross_val_score(xgb_pipeline, X_train, y_train, cv=cv, scoring = 'roc_auc')\nprint(\"roc_auc = %f (%f)\" % (scores.mean(), scores.std()))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def print_results_gridsearch(gridsearch, list_param1, list_param2, name_param1, name_param2):\n    # Checking the results from each run in the gridsearch: \n    means = gridsearch.cv_results_['mean_test_score']\n    stds = gridsearch.cv_results_['std_test_score']\n    params = gridsearch.cv_results_['params']\n    print(\"The results from each run in the gridsearch:\")\n    for mean, stdev, param in zip(means, stds, params):\n        print(\"roc_auc = %f (%f) with: %r\" % (mean, stdev, param))\n\n  # Checking the best performing model:\n    print(\"\\n\")\n    print(\"Best model: roc_auc = %f using %s\" % (gridsearch.best_score_, gridsearch.best_params_))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"param_grid = {\"model__subsample\": [0.8], \"model__colsample_bytree\": [0.8]\n              , \"model__learning_rate\": [0.0001, 0.001, 0.01, 0.1]\n              , \"model__n_estimators\": range(400,500,50)\n              }\n#instantiate the Grid Search:\ncv = StratifiedKFold(n_splits=5, shuffle=True, random_state=7) \ngrid_cv1 = GridSearchCV(xgb_pipeline\n                        , param_grid\n                        , n_jobs= -1 \n                        , cv = cv\n                        , scoring=\"roc_auc\") \n# Fit\n_ = grid_cv1.fit(X_train, y_train) \n# Checking the results from each run in the gridsearch:  \nprint_results_gridsearch(gridsearch=grid_cv1, list_param1 = param_grid[\"model__learning_rate\"], list_param2 = param_grid[\"model__n_estimators\"]\n                         , name_param1 = 'learning_rate' , name_param2 = 'n_estimators')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"param_grid = {\"model__subsample\": [0.8], \"model__colsample_bytree\": [0.8], \"model__learning_rate\": [0.01], \"model__n_estimators\": [250]\n              , 'model__max_depth': range(3,10,2)\n              , 'model__min_child_weight': range(1,6,2)\n             }\n#instantiate the Grid Search:\ncv = StratifiedKFold(n_splits=5, shuffle=True, random_state=7)\ngrid_cv2 = GridSearchCV(xgb_pipeline\n                        , param_grid\n                        , n_jobs=-1\n                        , cv=cv\n                        , scoring=\"roc_auc\") \n# Fit\n_ = grid_cv2.fit(X_train, y_train) \n# Checking the results from each run in the gridsearch:  \nprint_results_gridsearch(gridsearch=grid_cv2, list_param1 = param_grid[\"model__max_depth\"], list_param2 = param_grid[\"model__min_child_weight\"]\n                         , name_param1 = 'max_depth' , name_param2 = 'min_child_weight')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"param_grid = {\"model__learning_rate\": [0.01], \"model__n_estimators\": [250], 'model__max_depth': [5], 'model__min_child_weight': [1]\n              , 'model__subsample':[i/10.0 for i in range(4,10)]\n              , 'model__colsample_bytree':[i/10.0 for i in range(4,10)] \n             }\n#instantiate the Grid Search:\ncv = StratifiedKFold(n_splits=5, shuffle=True, random_state=7)\ngrid_cv3 = GridSearchCV(xgb_pipeline\n                        , param_grid\n                        , n_jobs=-1\n                        , cv=cv\n                        , scoring=\"roc_auc\") \n# Fit\n_ = grid_cv3.fit(X_train, y_train) \n# Checking the results from each run in the gridsearch:  \nprint_results_gridsearch(gridsearch=grid_cv3, list_param1 = param_grid[\"model__subsample\"], list_param2 = param_grid[\"model__colsample_bytree\"]\n                         , name_param1 = 'subsample' , name_param2 = 'colsample_bytree')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"param_grid = {\"model__learning_rate\": [0.01], \"model__n_estimators\": [250], 'model__max_depth': [5], 'model__min_child_weight': [1], 'model__subsample':[0.5], 'model__colsample_bytree':[0.9] \n              , \"model__gamma\": [i/10.0 for i in range(0,6)]\n              , \"model__reg_lambda\": [0, 0.5, 1, 1.5, 2, 3, 4.5]\n             }\n              \n#instantiate the Grid Search:\ncv = StratifiedKFold(n_splits=5, shuffle=True, random_state=7)\ngrid_cv4 = GridSearchCV(xgb_pipeline\n                        , param_grid\n                        , n_jobs=-1\n                        , cv=cv\n                        , scoring=\"roc_auc\") \n# Fit\n_ = grid_cv4.fit(X_train, y_train) \n# Checking the results from each run in the gridsearch:  \nprint_results_gridsearch(gridsearch=grid_cv4, list_param1 = param_grid[\"model__gamma\"], list_param2 = param_grid[\"model__reg_lambda\"]\n                         , name_param1 = 'Gamma' , name_param2 = 'Lambda')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"grid_cv4.best_params_","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xgb_pipeline.fit(X_train, y_train)\n# Predict:\ny_pred_meta = xgb_pipeline.predict(X_test)\ny_pred_prob_meta = xgb_pipeline.predict_proba(X_test)[::,1]\n# Evaluate:\nprint(\"roc_auc_score:\",metrics.roc_auc_score(y_test, y_pred_meta))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"param_grid = {\"model__subsample\": [0.8], \"model__colsample_bytree\": [0.8]\n              , \"model__learning_rate\": [0.0001, 0.001, 0.01, 0.1, 0.2, 0.3]\n              , \"model__n_estimators\": range(50,500,50)\n              }\n#instantiate the Grid Search:\ncv = StratifiedKFold(n_splits=5, shuffle=True, random_state=7) \nrandom_cv1 = RandomizedSearchCV(xgb_pipeline\n                        , param_grid\n                        , n_jobs= -1 \n                        , cv = cv\n                        , scoring=\"roc_auc\") \n# Fit\n_ = random_cv1.fit(X_train, y_train) \n# Checking the results from each run in the gridsearch:  \nprint_results_gridsearch(gridsearch=random_cv1, list_param1 = param_grid[\"model__learning_rate\"], list_param2 = param_grid[\"model__n_estimators\"]\n                         , name_param1 = 'learning_rate' , name_param2 = 'n_estimators')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"param_grid = {\"model__subsample\": [0.8], \"model__colsample_bytree\": [0.8]\n              , \"model__learning_rate\": [0.0001, 0.001, 0.01, 0.1, 0.2, 0.3]\n              , \"model__n_estimators\": range(50,500,50)\n              }\n#instantiate the Grid Search:\ncv = StratifiedKFold(n_splits=5, shuffle=True, random_state=7) \nbayes_cv1 = BayesSearchCV(xgb_pipeline\n                        , param_grid\n                        , n_jobs= -1 \n                        , cv = cv\n                        , scoring=\"roc_auc\") \n# Fit\n_ = bayes_cv1.fit(X_train, y_train) \n# Checking the results from each run in the gridsearch:  \nprint_results_gridsearch(gridsearch=bayes_cv1, list_param1 = param_grid[\"model__learning_rate\"], list_param2 = param_grid[\"model__n_estimators\"]\n                         , name_param1 = 'learning_rate' , name_param2 = 'n_estimators')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"param_grid = {\"model__subsample\": [0.8], \"model__colsample_bytree\": [0.8], \"model__learning_rate\": [0.01], \"model__n_estimators\": [250]\n              , 'model__max_depth': range(3,10,2)\n              , 'model__min_child_weight': range(1,6,2)\n             }\n#instantiate the Grid Search:\ncv = StratifiedKFold(n_splits=5, shuffle=True, random_state=7)\nrandom_cv2 = RandomizedSearchCV(xgb_pipeline\n                        , param_grid\n                        , n_jobs=-1\n                        , cv=cv\n                        , scoring=\"roc_auc\") \n# Fit\n_ = random_cv2.fit(X_train, y_train) \n# Checking the results from each run in the gridsearch:  \nprint_results_gridsearch(gridsearch=random_cv2, list_param1 = param_grid[\"model__max_depth\"], list_param2 = param_grid[\"model__min_child_weight\"]\n                         , name_param1 = 'max_depth' , name_param2 = 'min_child_weight')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"param_grid = {\"model__subsample\": [0.8], \"model__colsample_bytree\": [0.8], \"model__learning_rate\": [0.01], \"model__n_estimators\": [250]\n              , 'model__max_depth': range(3,10,2)\n              , 'model__min_child_weight': range(1,6,2)\n             }\n#instantiate the Grid Search:\ncv = StratifiedKFold(n_splits=5, shuffle=True, random_state=7)\nbayes_cv2 = BayesSearchCV(xgb_pipeline\n                        , param_grid\n                        , n_jobs=-1\n                        , cv=cv\n                        , scoring=\"roc_auc\") \n# Fit\n_ = bayes_cv2.fit(X_train, y_train) \n# Checking the results from each run in the gridsearch:  \nprint_results_gridsearch(gridsearch=bayes_cv2, list_param1 = param_grid[\"model__max_depth\"], list_param2 = param_grid[\"model__min_child_weight\"]\n                         , name_param1 = 'max_depth' , name_param2 = 'min_child_weight')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"param_grid = {\"model__learning_rate\": [0.01], \"model__n_estimators\": [250], 'model__max_depth': [5], 'model__min_child_weight': [1]\n              , 'model__subsample':[i/10.0 for i in range(4,10)]\n              , 'model__colsample_bytree':[i/10.0 for i in range(4,10)] \n             }\n#instantiate the Grid Search:\ncv = StratifiedKFold(n_splits=5, shuffle=True, random_state=7)\nrandom_cv3 = RandomizedSearchCV(xgb_pipeline\n                        , param_grid\n                        , n_jobs=-1\n                        , cv=cv\n                        , scoring=\"roc_auc\") \n# Fit\n_ = random_cv3.fit(X_train, y_train) \n# Checking the results from each run in the gridsearch:  \nprint_results_gridsearch(gridsearch=random_cv3, list_param1 = param_grid[\"model__subsample\"], list_param2 = param_grid[\"model__colsample_bytree\"]\n                         , name_param1 = 'subsample' , name_param2 = 'colsample_bytree')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"param_grid = {\"model__learning_rate\": [0.01], \"model__n_estimators\": [250], 'model__max_depth': [5], 'model__min_child_weight': [1]\n              , 'model__subsample':[i/10.0 for i in range(4,10)]\n              , 'model__colsample_bytree':[i/10.0 for i in range(4,10)] \n             }\n#instantiate the Grid Search:\ncv = StratifiedKFold(n_splits=5, shuffle=True, random_state=7)\nbayes_cv3 = BayesSearchCV(xgb_pipeline\n                        , param_grid\n                        , n_jobs=-1\n                        , cv=cv\n                        , scoring=\"roc_auc\") \n# Fit\n_ = bayes_cv3.fit(X_train, y_train) \n# Checking the results from each run in the gridsearch:  \nprint_results_gridsearch(gridsearch=bayes_cv3, list_param1 = param_grid[\"model__subsample\"], list_param2 = param_grid[\"model__colsample_bytree\"]\n                         , name_param1 = 'subsample' , name_param2 = 'colsample_bytree')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"param_grid = {\"model__learning_rate\": [0.01], \"model__n_estimators\": [250], 'model__max_depth': [5], 'model__min_child_weight': [1], 'model__subsample':[0.5], 'model__colsample_bytree':[0.9] \n              , \"model__gamma\": [i/10.0 for i in range(0,6)]\n              , \"model__reg_lambda\": [0, 0.5, 1, 1.5, 2, 3, 4.5]\n             }\n              \n#instantiate the Grid Search:\ncv = StratifiedKFold(n_splits=5, shuffle=True, random_state=7)\nrandom_cv4 = RandomizedSearchCV(xgb_pipeline\n                        , param_grid\n                        , n_jobs=-1\n                        , cv=cv\n                        , scoring=\"roc_auc\") \n# Fit\n_ = random_cv4.fit(X_train, y_train) \n# Checking the results from each run in the gridsearch:  \nprint_results_gridsearch(gridsearch=random_cv4, list_param1 = param_grid[\"model__gamma\"], list_param2 = param_grid[\"model__reg_lambda\"]\n                         , name_param1 = 'Gamma' , name_param2 = 'Lambda')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"param_grid = {\"model__learning_rate\": [0.01], \"model__n_estimators\": [250], 'model__max_depth': [5], 'model__min_child_weight': [1], 'model__subsample':[0.5], 'model__colsample_bytree':[0.9] \n              , \"model__gamma\": [i/10.0 for i in range(0,6)]\n              , \"model__reg_lambda\": [0, 0.5, 1, 1.5, 2, 3, 4.5]\n             }\n              \n#instantiate the Grid Search:\ncv = StratifiedKFold(n_splits=5, shuffle=True, random_state=7)\nbayes_cv4 = BayesSearchCV(xgb_pipeline\n                        , param_grid\n                        , n_jobs=-1\n                        , cv=cv\n                        , scoring=\"roc_auc\") \n# Fit\n_ = bayes_cv4.fit(X_train, y_train) \n# Checking the results from each run in the gridsearch:  \nprint_results_gridsearch(gridsearch=bayes_cv4, list_param1 = param_grid[\"model__gamma\"], list_param2 = param_grid[\"model__reg_lambda\"]\n                         , name_param1 = 'Gamma' , name_param2 = 'Lambda')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_test = pd.read_csv('/kaggle/input/cisc-873-dm-w23-a2/test.csv',index_col=0)","metadata":{"execution":{"iopub.status.busy":"2023-03-30T10:55:34.774688Z","iopub.execute_input":"2023-03-30T10:55:34.775271Z","iopub.status.idle":"2023-03-30T10:55:34.829249Z","shell.execute_reply.started":"2023-03-30T10:55:34.775224Z","shell.execute_reply":"2023-03-30T10:55:34.828226Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_test['mn_sat'] = pd.to_numeric(data_test['mn_sat'], errors='coerce')\ndata_test['tuition'] = pd.to_numeric(data_test['tuition'], errors='coerce')\ndata_test['zipcode'] = pd.to_numeric(data_test['zipcode'], errors='coerce')\ndata_test['income'] = pd.to_numeric(data_test['income'], errors='coerce')","metadata":{"execution":{"iopub.status.busy":"2023-03-30T10:55:35.067369Z","iopub.execute_input":"2023-03-30T10:55:35.068103Z","iopub.status.idle":"2023-03-30T10:55:35.095778Z","shell.execute_reply.started":"2023-03-30T10:55:35.068069Z","shell.execute_reply":"2023-03-30T10:55:35.094283Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_test['field'] = data_test['field'].astype(\"category\")\ndata_test['undergra'] = data_test['undergra'].astype(\"category\")\ndata_test['from'] = data_test['from'].astype(\"category\")\ndata_test['career'] = data_test['career'].astype(\"category\")","metadata":{"execution":{"iopub.status.busy":"2023-03-30T10:55:35.387144Z","iopub.execute_input":"2023-03-30T10:55:35.387536Z","iopub.status.idle":"2023-03-30T10:55:35.408546Z","shell.execute_reply.started":"2023-03-30T10:55:35.387503Z","shell.execute_reply":"2023-03-30T10:55:35.407583Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission = pd.DataFrame()\n\nsubmission['id'] = data_test['id']\n\nsubmission['match'] = grid_search.predict_proba(data_test)[:,1]\n\nsubmission.to_csv('sample_submission_walkthrough.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2023-03-30T10:55:36.110065Z","iopub.execute_input":"2023-03-30T10:55:36.110623Z","iopub.status.idle":"2023-03-30T10:55:36.186804Z","shell.execute_reply.started":"2023-03-30T10:55:36.110589Z","shell.execute_reply":"2023-03-30T10:55:36.185993Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"ðŸŒˆ Why a simple linear regression model (without any activation function) is not good for classification task, compared to Perceptron/Logistic regression?\nA simple linear regression model without any activation function is not good for classification tasks because it cannot model the relationship between the input features and the output classes in a way that allows for non-linear decision boundaries.\nIn contrast, Perceptron/Logistic regression models use activation functions such as the sigmoid function to model the probability of belonging to a particular class based on the input features. This allows them to capture non-linear relationships between the input features and the output classes and to make predictions that are more accurate than those made by simple linear regression models.\n\nðŸŒˆWhat's a decision tree and how it is different to a logistic regression model?\nA decision tree is a type of supervised learning algorithm that is mostly used for classification tasks. It works by recursively partitioning the input space into smaller regions based on the values of the input features until it reaches a stopping criterion, such as a maximum depth or minimum number of samples per leaf node.\nIn contrast, logistic regression is a linear model that models the probability of belonging to a particular class based on the input features. It works by fitting a line to the data that separates the two classes and then using this line to make predictions about new data points.\nThe main difference between decision trees and logistic regression models is that decision trees can capture non-linear relationships between the input features and the output classes, while logistic regression models can only capture linear relationships.\n\nðŸŒˆWhat's the difference between grid search and random search?\nGrid search and random search are two popular methods for hyperparameter tuning in machine learning.\nGrid search involves defining a grid of hyperparameters and then searching over all possible combinations of these hyperparameters to find the best model. This can be computationally expensive, especially when the number of hyperparameters is large.\nRandom search, on the other hand, involves randomly sampling hyperparameters from a predefined distribution and then evaluating the performance of the model with these hyperparameters. This can be much faster than grid search, especially when the number of hyperparameters is large.\nWhile grid search is guaranteed to find the optimal set of hyperparameters (assuming that the grid is fine enough), random search can be more efficient in practice because it can explore a larger portion of the hyperparameter space in less time.\n\nðŸŒˆWhat's the difference between bayesian search and random search?\nBayesian optimization is another method for hyperparameter tuning in machine learning.\nBayesian optimization uses a probabilistic model to approximate the objective function and then selects the next set of hyperparameters to evaluate based on an acquisition function that balances exploration and exploitation. This can be more efficient than random search because it can learn from previous evaluations and focus on promising regions of the hyperparameter space.\nA recent study has shown that Bayesian optimization is superior to random search for machine learning hyperparameter tuning1. However, itâ€™s worth noting that the performance of these methods can depend on the specific problem and dataset.","metadata":{}}]}